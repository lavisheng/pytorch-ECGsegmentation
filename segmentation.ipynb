{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import scipy.io as sio\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "# Graphing\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Tensorboard\n",
    "now = datetime.now()\n",
    "writer = SummaryWriter('./runs/ecgclassifier' + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "input_size = 1\n",
    "hidden_size = 200\n",
    "num_layers = 1\n",
    "num_classes = 4\n",
    "num_epoch = 10\n",
    "batch_size = 50\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the data on tensorboard\n",
    "ecg = sio.loadmat(\"./data/QTDataset/ecg1.mat\")[\"ecgSignal\"]\n",
    "for i, s in enumerate(ecg[100:500]):\n",
    "   writer.add_scalar('Sample Signal', s, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Custom dataset\n",
    "class ECGDataset(Dataset):\n",
    "   def __init__(self, folder_path):\n",
    "      \"\"\"\n",
    "      A dataset built off of ECG data in .mat form\n",
    "      Args:\n",
    "      folder_path (string): path to folder\n",
    "      \"\"\"\n",
    "      # get image paths \n",
    "      image_list = glob.glob(folder_path+'*.mat')\n",
    "      image_list.sort(key=lambda v: int(v[len(\"./data/QTDataset/ecg\"):-4]))\n",
    "      # get label paths\n",
    "      label_list = glob.glob(folder_path+'*.csv')\n",
    "      label_list.sort(key=lambda v: int(v[len(\"./data/QTDataset/ecg\"):-4]))\n",
    "      # Initialize the numpy arrays to store ecg and labels\n",
    "      # Iterate over the labels and image_list (assume that they are of same length)\n",
    "      for i in range(0, len(label_list)):\n",
    "         # Load mat then convert to numpy\n",
    "         ecgData = sio.loadmat(image_list[i])[\"ecgSignal\"]\n",
    "         ecgData = ecgData.flatten()\n",
    "         ecgData = ecgData.astype(np.float32)\n",
    "         ecgSize = ecgData.size\n",
    "         trim = -1 * (ecgSize % 5000)\n",
    "         # Reshape ecg into samples of 5000 ignoring whatever is left over\n",
    "         ecgData = ecgData[: trim if trim != 0 else ecgSize].reshape(ecgSize // 5000, 5000)\n",
    "         if i == 0:\n",
    "            self.ecgs = ecgData\n",
    "         else:\n",
    "            self.ecgs = np.concatenate((self.ecgs, ecgData), axis=0)\n",
    "         # handle the labels\n",
    "         labelData = pd.read_csv(label_list[i])\n",
    "         labels = np.array([0] * ecgSize)\n",
    "         # Encoders to convert P, T, QRS to expected output from model\n",
    "         encoder = {\n",
    "            'P': 1,\n",
    "            'T': 2,\n",
    "            'QRS':3\n",
    "            }\n",
    "         for _, row in labelData.iterrows():\n",
    "            labels[range(row['ROILimits_1'], row['ROILimits_2']+1)] = encoder[row['Value']]\n",
    "         labels = labels[:trim if trim != 0 else ecgSize].reshape(ecgSize // 5000, 5000)\n",
    "         if i == 0:\n",
    "            self.labels = labels\n",
    "         else:\n",
    "            self.labels = np.concatenate((self.labels, labels), axis=0)\n",
    "   def __getitem__(self, index):\n",
    "      return torch.from_numpy(self.ecgs[index]), torch.from_numpy(self.labels[index])\n",
    "   def __len__(self):\n",
    "      return len(self.ecgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "dataset = ECGDataset(r\"./data/QTDataset/\")\n",
    "validation_split = .3\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "class Classifier(nn.Module):\n",
    "   def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "      super(Classifier, self).__init__()\n",
    "      self.hidden_size = hidden_size\n",
    "      self.num_layers = num_layers\n",
    "      self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first = True)\n",
    "      self.fc = nn.Linear(hidden_size, num_classes)\n",
    "      self.softmax = nn.Softmax(dim=2)\n",
    "   def forward(self, x):\n",
    "      # Set initial hidden and cell states\n",
    "      h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "      c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "      lstm_out, _= self.lstm(x, (h0, c0))\n",
    "      data = self.fc(lstm_out)\n",
    "      out = self.softmax(data)\n",
    "      return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = Classifier(input_size, hidden_size, num_layers, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, eps=1e-3, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step[10/131] Loss: 1.2189\n",
      "Epoch [1/10], Step[20/131] Loss: 1.2265\n",
      "Epoch [1/10], Step[30/131] Loss: 1.2205\n",
      "Epoch [1/10], Step[40/131] Loss: 1.2089\n",
      "Epoch [1/10], Step[50/131] Loss: 1.1950\n",
      "Epoch [1/10], Step[60/131] Loss: 1.2073\n",
      "Epoch [1/10], Step[70/131] Loss: 1.1926\n",
      "Epoch [1/10], Step[80/131] Loss: 1.1616\n",
      "Epoch [1/10], Step[90/131] Loss: 1.1879\n",
      "Epoch [1/10], Step[100/131] Loss: 1.1347\n",
      "Epoch [1/10], Step[110/131] Loss: 1.1443\n",
      "Epoch [1/10], Step[120/131] Loss: 1.1645\n",
      "Epoch [1/10], Step[130/131] Loss: 1.1637\n",
      "Epoch [2/10], Step[10/131] Loss: 1.1392\n",
      "Epoch [2/10], Step[20/131] Loss: 1.1464\n",
      "Epoch [2/10], Step[30/131] Loss: 1.1398\n",
      "Epoch [2/10], Step[40/131] Loss: 1.1560\n",
      "Epoch [2/10], Step[50/131] Loss: 1.1607\n",
      "Epoch [2/10], Step[60/131] Loss: 1.1417\n",
      "Epoch [2/10], Step[70/131] Loss: 1.1587\n",
      "Epoch [2/10], Step[80/131] Loss: 1.1261\n",
      "Epoch [2/10], Step[90/131] Loss: 1.1472\n",
      "Epoch [2/10], Step[100/131] Loss: 1.1197\n",
      "Epoch [2/10], Step[110/131] Loss: 1.0969\n",
      "Epoch [2/10], Step[120/131] Loss: 1.0842\n",
      "Epoch [2/10], Step[130/131] Loss: 1.1229\n",
      "Epoch [3/10], Step[10/131] Loss: 1.1448\n",
      "Epoch [3/10], Step[20/131] Loss: 1.0912\n",
      "Epoch [3/10], Step[30/131] Loss: 1.0996\n",
      "Epoch [3/10], Step[40/131] Loss: 1.1404\n",
      "Epoch [3/10], Step[50/131] Loss: 1.1118\n",
      "Epoch [3/10], Step[60/131] Loss: 1.1063\n",
      "Epoch [3/10], Step[70/131] Loss: 1.1328\n",
      "Epoch [3/10], Step[80/131] Loss: 1.1125\n",
      "Epoch [3/10], Step[90/131] Loss: 1.0792\n",
      "Epoch [3/10], Step[100/131] Loss: 1.0937\n",
      "Epoch [3/10], Step[110/131] Loss: 1.0751\n",
      "Epoch [3/10], Step[120/131] Loss: 1.0396\n",
      "Epoch [3/10], Step[130/131] Loss: 1.0572\n",
      "Epoch [4/10], Step[10/131] Loss: 1.0650\n",
      "Epoch [4/10], Step[20/131] Loss: 1.0648\n",
      "Epoch [4/10], Step[30/131] Loss: 1.1160\n",
      "Epoch [4/10], Step[40/131] Loss: 1.0575\n",
      "Epoch [4/10], Step[50/131] Loss: 1.0433\n",
      "Epoch [4/10], Step[60/131] Loss: 1.0829\n",
      "Epoch [4/10], Step[70/131] Loss: 1.0562\n",
      "Epoch [4/10], Step[80/131] Loss: 1.0778\n",
      "Epoch [4/10], Step[90/131] Loss: 1.0412\n",
      "Epoch [4/10], Step[100/131] Loss: 1.0521\n",
      "Epoch [4/10], Step[110/131] Loss: 1.0569\n",
      "Epoch [4/10], Step[120/131] Loss: 1.0933\n",
      "Epoch [4/10], Step[130/131] Loss: 1.0337\n",
      "Epoch [5/10], Step[10/131] Loss: 1.0450\n",
      "Epoch [5/10], Step[20/131] Loss: 1.0158\n",
      "Epoch [5/10], Step[30/131] Loss: 1.0495\n",
      "Epoch [5/10], Step[40/131] Loss: 1.0628\n",
      "Epoch [5/10], Step[50/131] Loss: 1.0604\n",
      "Epoch [5/10], Step[60/131] Loss: 1.0845\n",
      "Epoch [5/10], Step[70/131] Loss: 1.0606\n",
      "Epoch [5/10], Step[80/131] Loss: 1.0384\n",
      "Epoch [5/10], Step[90/131] Loss: 1.0331\n",
      "Epoch [5/10], Step[100/131] Loss: 1.0749\n",
      "Epoch [5/10], Step[110/131] Loss: 1.0844\n",
      "Epoch [5/10], Step[120/131] Loss: 1.0561\n",
      "Epoch [5/10], Step[130/131] Loss: 1.0756\n",
      "Epoch [6/10], Step[10/131] Loss: 1.0581\n",
      "Epoch [6/10], Step[20/131] Loss: 1.0077\n",
      "Epoch [6/10], Step[30/131] Loss: 1.0562\n",
      "Epoch [6/10], Step[40/131] Loss: 1.0593\n",
      "Epoch [6/10], Step[50/131] Loss: 1.0394\n",
      "Epoch [6/10], Step[60/131] Loss: 1.0204\n",
      "Epoch [6/10], Step[70/131] Loss: 1.0735\n",
      "Epoch [6/10], Step[80/131] Loss: 1.0437\n",
      "Epoch [6/10], Step[90/131] Loss: 1.0609\n",
      "Epoch [6/10], Step[100/131] Loss: 1.1113\n",
      "Epoch [6/10], Step[110/131] Loss: 1.0567\n",
      "Epoch [6/10], Step[120/131] Loss: 1.0271\n",
      "Epoch [6/10], Step[130/131] Loss: 1.0540\n",
      "Epoch [7/10], Step[10/131] Loss: 1.0508\n",
      "Epoch [7/10], Step[20/131] Loss: 1.0700\n",
      "Epoch [7/10], Step[30/131] Loss: 1.0397\n",
      "Epoch [7/10], Step[40/131] Loss: 1.0534\n",
      "Epoch [7/10], Step[50/131] Loss: 1.0700\n",
      "Epoch [7/10], Step[60/131] Loss: 1.0637\n",
      "Epoch [7/10], Step[70/131] Loss: 1.0407\n",
      "Epoch [7/10], Step[80/131] Loss: 1.0663\n",
      "Epoch [7/10], Step[90/131] Loss: 1.0928\n",
      "Epoch [7/10], Step[100/131] Loss: 1.0893\n",
      "Epoch [7/10], Step[110/131] Loss: 1.0232\n",
      "Epoch [7/10], Step[120/131] Loss: 1.0533\n",
      "Epoch [7/10], Step[130/131] Loss: 1.0432\n",
      "Epoch [8/10], Step[10/131] Loss: 1.0602\n",
      "Epoch [8/10], Step[20/131] Loss: 1.0731\n",
      "Epoch [8/10], Step[30/131] Loss: 1.0408\n",
      "Epoch [8/10], Step[40/131] Loss: 1.0513\n",
      "Epoch [8/10], Step[50/131] Loss: 1.0160\n",
      "Epoch [8/10], Step[60/131] Loss: 1.0485\n",
      "Epoch [8/10], Step[70/131] Loss: 1.0773\n",
      "Epoch [8/10], Step[80/131] Loss: 1.0756\n",
      "Epoch [8/10], Step[90/131] Loss: 1.1003\n",
      "Epoch [8/10], Step[100/131] Loss: 1.0900\n",
      "Epoch [8/10], Step[110/131] Loss: 1.0791\n",
      "Epoch [8/10], Step[120/131] Loss: 1.0836\n",
      "Epoch [8/10], Step[130/131] Loss: 1.0658\n",
      "Epoch [9/10], Step[10/131] Loss: 1.0703\n",
      "Epoch [9/10], Step[20/131] Loss: 1.0486\n",
      "Epoch [9/10], Step[30/131] Loss: 0.9947\n",
      "Epoch [9/10], Step[40/131] Loss: 1.0595\n",
      "Epoch [9/10], Step[50/131] Loss: 1.0520\n",
      "Epoch [9/10], Step[60/131] Loss: 1.0762\n",
      "Epoch [9/10], Step[70/131] Loss: 1.0550\n",
      "Epoch [9/10], Step[80/131] Loss: 1.0624\n",
      "Epoch [9/10], Step[90/131] Loss: 1.1015\n",
      "Epoch [9/10], Step[100/131] Loss: 1.0693\n",
      "Epoch [9/10], Step[110/131] Loss: 1.0748\n",
      "Epoch [9/10], Step[120/131] Loss: 1.0423\n",
      "Epoch [9/10], Step[130/131] Loss: 1.0222\n",
      "Epoch [10/10], Step[10/131] Loss: 1.0448\n",
      "Epoch [10/10], Step[20/131] Loss: 1.0745\n",
      "Epoch [10/10], Step[30/131] Loss: 1.1109\n",
      "Epoch [10/10], Step[40/131] Loss: 1.0840\n",
      "Epoch [10/10], Step[50/131] Loss: 1.1195\n",
      "Epoch [10/10], Step[60/131] Loss: 1.0612\n",
      "Epoch [10/10], Step[70/131] Loss: 1.1289\n",
      "Epoch [10/10], Step[80/131] Loss: 1.1043\n",
      "Epoch [10/10], Step[90/131] Loss: 1.0574\n",
      "Epoch [10/10], Step[100/131] Loss: 1.0861\n",
      "Epoch [10/10], Step[110/131] Loss: 1.0490\n",
      "Epoch [10/10], Step[120/131] Loss: 1.0635\n",
      "Epoch [10/10], Step[130/131] Loss: 1.0615\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Train the model:\n",
    "losses = []\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epoch):\n",
    "   for i, (samples, labels) in enumerate(train_loader):\n",
    "      model.zero_grad()\n",
    "\n",
    "      samples = samples.reshape(-1, 5000, input_size).to(device)\n",
    "      # reshape labels to match future output\n",
    "      labels = labels.view(-1)\n",
    "      labels = labels.to(device)\n",
    "      \n",
    "      # fwd pass\n",
    "      outputs = model(samples)\n",
    "      # Reshape to (batch_size * seq_length, C)\n",
    "      outputs = outputs.view(-1, outputs.size(2))\n",
    "\n",
    "      # Backwards\n",
    "      loss = criterion(outputs, labels)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # Write training loss\n",
    "      writer.add_scalar('Training Loss', loss.item(), (epoch * total_step) + (i))\n",
    "      # Write training accuracy\n",
    "      _, predicted = torch.max(outputs, 1)\n",
    "      correct = (predicted == labels).sum().item()\n",
    "      writer.add_scalar('Training Accuracy', 100 * (correct /  labels.size(0)),\n",
    "                        (epoch * total_step) + (i))\n",
    "      if (i+1) % 10 == 0:\n",
    "         print(\"Epoch [{}/{}], Step[{}/{}] Loss: {:.4f}\"\n",
    "               .format(epoch+1, num_epoch, i+1, total_step, loss.item()))\n",
    "   # Calculate validation loss\n",
    "   model.eval()\n",
    "   with torch.no_grad():\n",
    "      loss = 0\n",
    "      for samples, labels in validation_loader:\n",
    "         samples = samples.reshape(-1, 5000, input_size).to(device)\n",
    "         labels = labels.view(-1).to(device)\n",
    "         outputs = model(samples)\n",
    "         outputs = outputs.view(-1, outputs.size(2))\n",
    "\n",
    "         # Calculate loss and write to tensorboard\n",
    "         loss += criterion(outputs, labels).item()\n",
    "      writer.add_scalar('Validation Loss', loss, epoch)\n",
    "   model.train()\n",
    "print(\"Finished Training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Save model based off of time\n",
    "torch.save(model.state_dict(), 'model' + now.strftime(\"%Y%m%d-%H%M%S\") + '.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model.load_state_dict(torch.load('model' + now.strftime(\"%Y%m%d-%H%M%S\") + '.ckpt')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of the model on 14015000 samples: 68.5483481983589%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "model.eval()\n",
    "trueWriter = SummaryWriter('./runs/ecgclassifierTrue' + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\")\n",
    "predictWriter = SummaryWriter('./runs/ecgclassifierPredict' + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\")\n",
    "sampleCounter = 0\n",
    "with torch.no_grad():\n",
    "   correct = 0\n",
    "   total = 0\n",
    "   for i, (samples, labels) in enumerate(validation_loader):\n",
    "      samples = samples.reshape(-1, 5000, input_size).to(device)\n",
    "      labels = labels.view(-1).to(device)\n",
    "      outputs = model(samples)\n",
    "      outputs = outputs.view(-1, outputs.size(2))\n",
    "\n",
    "      # Calculate loss and write to tensorboard\n",
    "      loss = criterion(outputs, labels)\n",
    "      writer.add_scalar('Validation Loss', loss.item(), i)\n",
    "\n",
    "      # Calculate correct and increment total\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "      batch_correct = (predicted == labels).sum().item()\n",
    "      writer.add_scalar('Validation Accuracy', batch_correct / labels.size(0), i)\n",
    "      total += labels.size(0)\n",
    "      correct += batch_correct\n",
    "   print('Test accuracy of the model on {} samples: {}%'\n",
    "         .format(total, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# helper function to graph\n",
    "def label2colour(labels):\n",
    "   colors = np.zeros(shape=(labels.size(0), 4))\n",
    "   encoder = {\n",
    "      0: tuple([1,1,1,1]),\n",
    "      1: tuple([1,0,0,1]),\n",
    "      2: tuple([0,1,0,1]),\n",
    "      3: tuple([0,0,1,1])\n",
    "   }\n",
    "\n",
    "   for i, l in enumerate(labels):\n",
    "      colors[i] = encoder[l]\n",
    "   return colors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/home/sweaterprincess/miniconda3/bin/python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "segmentation.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
