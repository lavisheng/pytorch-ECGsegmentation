{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import scipy.io as sio\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "# Graphing\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Tensorboard\n",
    "now = datetime.now()\n",
    "writer = SummaryWriter('./runs/ecgclassifier' + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "input_size = 1\n",
    "hidden_size = 200\n",
    "num_layers = 1\n",
    "num_classes = 4\n",
    "num_epoch = 10\n",
    "batch_size = 50\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the data on tensorboard\n",
    "ecg = sio.loadmat(\"./data/QTDataset/ecg1.mat\")[\"ecgSignal\"]\n",
    "for i, s in enumerate(ecg[100:500]):\n",
    "   writer.add_scalar('Sample Signal', s, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Custom dataset\n",
    "class ECGDataset(Dataset):\n",
    "   def __init__(self, folder_path):\n",
    "      \"\"\"\n",
    "      A dataset built off of ECG data in .mat form\n",
    "      Args:\n",
    "      folder_path (string): path to folder\n",
    "      \"\"\"\n",
    "      # get image paths \n",
    "      image_list = glob.glob(folder_path+'*.mat')\n",
    "      image_list.sort(key=lambda v: int(v[len(\"./data/QTDataset/ecg\"):-4]))\n",
    "      # get label paths\n",
    "      label_list = glob.glob(folder_path+'*.csv')\n",
    "      label_list.sort(key=lambda v: int(v[len(\"./data/QTDataset/ecg\"):-4]))\n",
    "      # Initialize the numpy arrays to store ecg and labels\n",
    "      # Iterate over the labels and image_list (assume that they are of same length)\n",
    "      for i in range(0, len(label_list)):\n",
    "         # Load mat then convert to numpy\n",
    "         ecgData = sio.loadmat(image_list[i])[\"ecgSignal\"]\n",
    "         ecgData = ecgData.flatten()\n",
    "         ecgData = ecgData.astype(np.float32)\n",
    "         ecgSize = ecgData.size\n",
    "         trim = -1 * (ecgSize % 5000)\n",
    "         # Reshape ecg into samples of 5000 ignoring whatever is left over\n",
    "         ecgData = ecgData[: trim if trim != 0 else ecgSize].reshape(ecgSize // 5000, 5000)\n",
    "         if i == 0:\n",
    "            self.ecgs = ecgData\n",
    "         else:\n",
    "            self.ecgs = np.concatenate((self.ecgs, ecgData), axis=0)\n",
    "         # handle the labels\n",
    "         labelData = pd.read_csv(label_list[i])\n",
    "         labels = np.array([0] * ecgSize)\n",
    "         # Encoders to convert P, T, QRS to expected output from model\n",
    "         encoder = {\n",
    "            'P': 1,\n",
    "            'T': 2,\n",
    "            'QRS':3\n",
    "            }\n",
    "         for _, row in labelData.iterrows():\n",
    "            labels[range(row['ROILimits_1'], row['ROILimits_2']+1)] = encoder[row['Value']]\n",
    "         labels = labels[:trim if trim != 0 else ecgSize].reshape(ecgSize // 5000, 5000)\n",
    "         if i == 0:\n",
    "            self.labels = labels\n",
    "         else:\n",
    "            self.labels = np.concatenate((self.labels, labels), axis=0)\n",
    "   def __getitem__(self, index):\n",
    "      return torch.from_numpy(self.ecgs[index]), torch.from_numpy(self.labels[index])\n",
    "   def __len__(self):\n",
    "      return len(self.ecgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "dataset = ECGDataset(r\"./data/QTDataset/\")\n",
    "validation_split = .3\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "class Classifier(nn.Module):\n",
    "   def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "      super(Classifier, self).__init__()\n",
    "      self.hidden_size = hidden_size\n",
    "      self.num_layers = num_layers\n",
    "      self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first = True)\n",
    "      self.fc = nn.Linear(hidden_size, num_classes)\n",
    "      self.softmax = nn.Softmax(dim=2)\n",
    "   def forward(self, x):\n",
    "      # Set initial hidden and cell states\n",
    "      h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "      c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "      lstm_out, _= self.lstm(x, (h0, c0))\n",
    "      data = self.fc(lstm_out)\n",
    "      out = self.softmax(data)\n",
    "      return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = Classifier(input_size, hidden_size, num_layers, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, eps=1e-3, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Train the model:\n",
    "losses = []\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epoch):\n",
    "   for i, (samples, labels) in enumerate(train_loader):\n",
    "      model.zero_grad()\n",
    "\n",
    "      samples = samples.reshape(-1, 5000, input_size).to(device)\n",
    "      # reshape labels to match future output\n",
    "      labels = labels.view(-1)\n",
    "      labels = labels.to(device)\n",
    "      \n",
    "      # fwd pass\n",
    "      outputs = model(samples)\n",
    "      # Reshape to (batch_size * seq_length, C)\n",
    "      outputs = outputs.view(-1, outputs.size(2))\n",
    "\n",
    "      # Backwards\n",
    "      loss = criterion(outputs, labels)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # Write training loss\n",
    "      writer.add_scalar('Training Loss', loss.item(), (epoch * total_step) + (i))\n",
    "      # Write training accuracy\n",
    "      _, predicted = torch.max(outputs, 1)\n",
    "      correct = (predicted == labels).sum().item()\n",
    "      writer.add_scalar('Training Accuracy', 100 * (correct /  labels.size(0)),\n",
    "                        (epoch * total_step) + (i))\n",
    "      if (i+1) % 10 == 0:\n",
    "         print(\"Epoch [{}/{}], Step[{}/{}] Loss: {:.4f}\"\n",
    "               .format(epoch+1, num_epoch, i+1, total_step, loss.item()))\n",
    "   # Calculate validation loss\n",
    "   model.eval()\n",
    "   with torch.no_grad():\n",
    "      loss = 0\n",
    "      for samples, labels in validation_loader:\n",
    "         samples = samples.reshape(-1, 5000, input_size).to(device)\n",
    "         labels = labels.view(-1).to(device)\n",
    "         outputs = model(samples)\n",
    "         outputs = outputs.view(-1, outputs.size(2))\n",
    "\n",
    "         # Calculate loss and write to tensorboard\n",
    "         loss += criterion(outputs, labels).item()\n",
    "      writer.add_scalar('Validation Loss', loss, epoch)\n",
    "   model.train()\n",
    "print(\"Finished Training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Save model based off of time\n",
    "torch.save(model.state_dict(), 'model' + now.strftime(\"%Y%m%d-%H%M%S\") + '.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model.load_state_dict(torch.load('./to_validate/' + 'model' + '20200429-162615' + '.ckpt')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of the model on 14015000 samples: 68.5483481983589%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "model.eval()\n",
    "sampleCounter = 0\n",
    "total_labels = []\n",
    "total_predicted = []\n",
    "total_signal = []\n",
    "signal_counter = 0\n",
    "with torch.no_grad():\n",
    "   correct = 0\n",
    "   total = 0\n",
    "   for i, (samples, labels) in enumerate(validation_loader):\n",
    "      samples = samples.reshape(-1, 5000, input_size).to(device)\n",
    "      labels = labels.view(-1).to(device)\n",
    "      outputs = model(samples)\n",
    "      outputs = outputs.view(-1, outputs.size(2))\n",
    "      # Calculate loss and write to tensorboard\n",
    "      loss = criterion(outputs, labels)\n",
    "      writer.add_scalar('Validation Loss', loss.item(), i)\n",
    "\n",
    "      # Calculate correct and increment total\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "      batch_correct = (predicted == labels).sum().item()\n",
    "      writer.add_scalar('Validation Accuracy', batch_correct / labels.size(0), i)\n",
    "      total += labels.size(0)\n",
    "      correct += batch_correct\n",
    "\n",
    "      signal = np.dstack((\n",
    "         np.arange(signal_counter, labels.size(0) + signal_counter),\n",
    "         samples.flatten()))\n",
    "      #print(signal)\n",
    "      signal_counter += labels.size(0)\n",
    "      # Save labels, predicted, and signal for graphing later\n",
    "      if i == 0:\n",
    "         total_signal = signal\n",
    "         total_labels = labels\n",
    "         total_predicted = predicted\n",
    "      else:\n",
    "         total_signal = np.concatenate((total_signal, signal), axis=1)\n",
    "         total_labels = np.concatenate((total_labels, labels))\n",
    "         total_predicted = np.concatenate((total_predicted, predicted))\n",
    "   # Squeeze to remove an extra dimension\n",
    "   total_signal = total_signal.squeeze()\n",
    "   # hstack to form the segments\n",
    "   total_signal = np.hstack([total_signal[:-1], total_signal[1:])\n",
    "   print('Test accuracy of the model on {} samples: {}%'\n",
    "         .format(total, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# helper function to graph\n",
    "def label2colour(labels, alpha):\n",
    "   colors = np.zeros(shape=(labels.size, 4))\n",
    "   encoder = {\n",
    "      0: tuple([0,0,0,alpha]),\n",
    "      1: tuple([1,0,0,alpha]),\n",
    "      2: tuple([0,1,0,alpha]),\n",
    "      3: tuple([0,0,1,alpha])\n",
    "   }\n",
    "\n",
    "   for i, l in enumerate(labels):\n",
    "      colors[i] = encoder[l]\n",
    "   return colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Graph using data from validation\n",
    "disp_signal= total_signals[0:10000]\n",
    "disp_predicted = total_predicted[0:20000]\n",
    "disp_labels = total_labels[0:20000]\n",
    "predicted_lc = LineCollection(segments=disp_signal,\n",
    "                              colors=label2colour(disp_predicted, 1),\n",
    "                              linewidths=1)\n",
    "# High ram usage, freeing to save \n",
    "true_lc = LineCollection(segments=total_signalss,\n",
    "                         colors=label2colour(disp_labels, 0.2),\n",
    "                         linewidths=10)\n",
    "fig, ax = plt.subplots()\n",
    "# Add legend\n",
    "NA_patch = mpatches.Patch(color='black', label='NA')\n",
    "P_patch = mpatches.Patch(color='red', label='P')\n",
    "T_patch = mpatches.Patch(color='green', label='T')\n",
    "QRS_patch = mpatches.Patch(color='blue', label='QRS')\n",
    "plt.legend(handles=[NA_patch, P_patch, T_patch, QRS_patch])\n",
    "fig.set_size_inches(30, 22)\n",
    "ax.add_collection(true_lc)\n",
    "ax.add_collection(predicted_lc)\n",
    "ax.autoscale()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save\n",
    "fig.savefig('model_results.png', dpi = 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/home/sweaterprincess/miniconda3/bin/python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "segmentation.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
